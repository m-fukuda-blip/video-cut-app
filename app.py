import streamlit as st
import os
import cv2
import base64
import pandas as pd
import shutil
import zipfile
import io
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
from moviepy.editor import VideoFileClip
from openai import OpenAI, RateLimitError

# --- è¨­å®š ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(BASE_DIR, "temp_data")

def init_temp_dir():
    """ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆæœŸåŒ–"""
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
        except:
            pass
    os.makedirs(TEMP_DIR)

def encode_image(image_path):
    """ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def detect_scenes(video_path, threshold=27.0):
    """ã‚·ãƒ¼ãƒ³æ¤œå‡º"""
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=threshold))
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list(video_manager)
    return scene_list

def create_zip_file(data_list):
    """CSVã¨ç”»åƒã‚’ã¾ã¨ã‚ã¦ZIPã«ã™ã‚‹"""
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        # 1. CSVã‚’ä½œæˆã—ã¦è¿½åŠ 
        df = pd.DataFrame(data_list)
        # CSVå†…ã§ã¯ç”»åƒãƒ‘ã‚¹ã§ã¯ãªããƒ•ã‚¡ã‚¤ãƒ«åã ã‘ã«ã™ã‚‹
        df["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å"] = df["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"].apply(lambda x: os.path.basename(x))
        csv_data = df.drop(columns=["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]).to_csv(index=False).encode('utf-8')
        zf.writestr("cut_list.csv", csv_data)
        
        # 2. ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
        for row in data_list:
            img_path = row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]
            if os.path.exists(img_path):
                # ZIPå†…ã® images/ ãƒ•ã‚©ãƒ«ãƒ€ã«å…¥ã‚Œã‚‹
                zf.write(img_path, arcname=f"images/{os.path.basename(img_path)}")
                
    return zip_buffer.getvalue()

def process_video_and_analyze(api_key, video_file, max_scenes=10):
    client = OpenAI(api_key=api_key)
    init_temp_dir()

    video_path = os.path.join(TEMP_DIR, "input_video.mp4")
    try:
        with open(video_path, "wb") as f:
            f.write(video_file.read())
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    st.info("âœ‚ï¸ ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­... (æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")
    try:
        scenes = detect_scenes(video_path)
    except Exception as e:
        st.error(f"ã‚·ãƒ¼ãƒ³æ¤œå‡ºå¤±æ•—: {e}")
        return []

    st.write(f"åˆè¨ˆ **{len(scenes)}** ã‚«ãƒƒãƒˆæ¤œå‡ºã€‚")
    
    if len(scenes) > max_scenes:
        st.warning(f"âš ï¸ ãƒ‡ãƒ¢åˆ¶é™: æœ€åˆã® {max_scenes} ã‚«ãƒƒãƒˆã®ã¿å‡¦ç†ã—ã¾ã™ã€‚")
        scenes = scenes[:max_scenes]

    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    full_clip = VideoFileClip(video_path)

    for i, scene in enumerate(scenes):
        start_t = scene[0].get_seconds()
        end_t = scene[1].get_seconds()
        duration = end_t - start_t
        
        if duration < 0.5:
            continue

        status_text.text(f"AIåˆ†æä¸­: ã‚«ãƒƒãƒˆ {i+1}/{len(scenes)}")
        
        # ç”»åƒä¿å­˜
        thumb_filename = f"cut_{i+1:03}.jpg"
        thumb_path = os.path.join(TEMP_DIR, thumb_filename)
        mid_point = start_t + (duration / 2)
        
        try:
            full_clip.save_frame(thumb_path, t=mid_point)
        except:
            continue

        # éŸ³å£°å‡¦ç†
        audio_path = os.path.join(TEMP_DIR, f"audio_{i}.mp3")
        sub_clip = full_clip.subclip(start_t, end_t)
        transcript_text = "ï¼ˆãªã—ï¼‰"

        # Whisper (éŸ³å£°)
        if sub_clip.audio is not None:
            try:
                sub_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
                with open(audio_path, "rb") as audio_file:
                    transcription = client.audio.transcriptions.create(
                        model="whisper-1", file=audio_file, language="ja"
                    )
                transcript_text = transcription.text if transcription.text else "ï¼ˆãªã—ï¼‰"
            except RateLimitError:
                transcript_text = "âŒã€ã‚¨ãƒ©ãƒ¼ã€‘OpenAIã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆæ®‹é«˜ä¸è¶³ã§ã™"
            except Exception as e:
                transcript_text = "ï¼ˆéŸ³å£°ã‚¨ãƒ©ãƒ¼ï¼‰"

        # GPT-4o (ç”»åƒ)
        base64_image = encode_image(thumb_path)
        prompt = f"æ–‡å­—èµ·ã“ã—:ã€Œ{transcript_text}ã€ã€‚ã“ã®ã‚«ãƒƒãƒˆã®çŠ¶æ³ã¨æ„å›³ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ã€‚"
        
        analysis = ""
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                    ]}
                ],
                max_tokens=200
            )
            analysis = response.choices[0].message.content
        except RateLimitError:
            analysis = "âŒã€ã‚¨ãƒ©ãƒ¼ã€‘OpenAIã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆæ®‹é«˜ä¸è¶³ã§ã™ã€‚Billingè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            st.error("âš ï¸ OpenAIã®APIåˆ©ç”¨æ ã‚’è¶…éã—ã¾ã—ãŸï¼ˆError 429ï¼‰ã€‚ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            analysis = f"ã‚¨ãƒ©ãƒ¼: {e}"

        results.append({
            "ã‚«ãƒƒãƒˆNo": i+1,
            "é–‹å§‹": scene[0].get_timecode(),
            "çµ‚äº†": scene[1].get_timecode(),
            "ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹": thumb_path,
            "ã‚»ãƒªãƒ•": transcript_text,
            "AIåˆ†æ": analysis
        })
        progress_bar.progress((i + 1) / len(scenes))

    full_clip.close()
    status_text.text("å®Œäº†ï¼")
    return results

# --- UI ---
st.set_page_config(page_title="AIã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼", layout="wide")
st.title("ğŸ¬ AIæ˜ åƒã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼")

with st.sidebar:
    api_key = st.text_input("OpenAI API Key", type="password")
    st.info("ğŸ’¡ Error 429ãŒå‡ºãŸã‚‰: OpenAIã®Billingè¨­å®šã§ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆæ®‹é«˜ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    threshold = st.slider("ã‚«ãƒƒãƒˆæ¤œå‡ºæ„Ÿåº¦", 10.0, 60.0, 27.0)
    max_scenes_limit = st.number_input("æœ€å¤§åˆ†ææ•°", 5, 50, 5)

uploaded_file = st.file_uploader("å‹•ç”» (MP4)", type=['mp4', 'mov'])

if uploaded_file and api_key:
    if st.button("ğŸš€ åˆ†æã‚¹ã‚¿ãƒ¼ãƒˆ"):
        data = process_video_and_analyze(api_key, uploaded_file, max_scenes_limit)
        
        if data:
            st.success("åˆ†æå®Œäº†ï¼ä¸‹ã®ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
            
            # ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ä½œæˆ
            zip_bytes = create_zip_file(data)
            st.download_button(
                label="ğŸ“¦ çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV + ç”»åƒZIP)",
                data=zip_bytes,
                file_name="cut_analysis_result.zip",
                mime="application/zip"
            )

            # ç”»é¢è¡¨ç¤º
            for row in data:
                col1, col2, col3 = st.columns([2, 2, 4])
                with col1:
                    st.image(row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"])
                    st.caption(f"{row['é–‹å§‹']} - {row['çµ‚äº†']}")
                with col2:
                    st.write(f"ğŸ—£ {row['ã‚»ãƒªãƒ•']}")
                with col3:
                    st.write(f"ğŸ¤– {row['AIåˆ†æ']}")
                st.divider()
