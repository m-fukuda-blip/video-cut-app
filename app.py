import streamlit as st
import os
import cv2
import base64
import pandas as pd
import shutil
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
from moviepy.editor import VideoFileClip
from openai import OpenAI

# --- è¨­å®š ---
TEMP_DIR = "temp_data"  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€

def init_temp_dir():
    """ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ"""
    if not os.path.exists(TEMP_DIR):
        os.makedirs(TEMP_DIR)

def clear_temp_dir():
    """ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒªã‚»ãƒƒãƒˆ"""
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
        os.makedirs(TEMP_DIR)

def encode_image(image_path):
    """ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦GPTã«é€ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def detect_scenes(video_path, threshold=27.0):
    """ã‚·ãƒ¼ãƒ³ã®åˆ‡ã‚Šæ›¿ã‚ã‚Šæ™‚é–“ã‚’æ¤œå‡º"""
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=threshold))
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list(video_manager)
    return scene_list

def process_video_and_analyze(api_key, video_file, max_scenes=10):
    """å‹•ç”»å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯"""
    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    client = OpenAI(api_key=api_key)
    
    # ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã®æº–å‚™
    clear_temp_dir()

    # å‹•ç”»ã‚’ä¸€æ™‚ä¿å­˜
    video_path = os.path.join(TEMP_DIR, "input_video.mp4")
    with open(video_path, "wb") as f:
        f.write(video_file.read())

    st.info("âœ‚ï¸ ã‚·ãƒ¼ãƒ³ï¼ˆã‚«ãƒƒãƒˆï¼‰ã®æ¤œå‡ºä¸­... å‹•ç”»ã®é•·ã•ã«ã‚ˆã£ã¦ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚")
    
    # ã‚·ãƒ¼ãƒ³æ¤œå‡ºå®Ÿè¡Œ
    try:
        scenes = detect_scenes(video_path)
    except Exception as e:
        st.error(f"ã‚·ãƒ¼ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return []

    st.write(f"åˆè¨ˆ **{len(scenes)}** å€‹ã®ã‚«ãƒƒãƒˆã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")

    if len(scenes) > max_scenes:
        st.warning(f"âš ï¸ ãƒ‡ãƒ¢ã®ãŸã‚ã€æœ€åˆã® {max_scenes} ã‚«ãƒƒãƒˆã®ã¿åˆ†æã—ã¾ã™ã€‚")
        scenes = scenes[:max_scenes]

    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()

    # å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã®èª­ã¿è¾¼ã¿
    full_clip = VideoFileClip(video_path)

    for i, scene in enumerate(scenes):
        start_t = scene[0].get_seconds()
        end_t = scene[1].get_seconds()
        duration = end_t - start_t
        
        # æ¥µç«¯ã«çŸ­ã„ã‚«ãƒƒãƒˆï¼ˆ0.5ç§’æœªæº€ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
        if duration < 0.5:
            continue

        status_text.text(f"åˆ†æä¸­: ã‚«ãƒƒãƒˆ {i+1} / {len(scenes)}")
        
        # --- 1. ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®ä¿å­˜ ---
        thumb_path = os.path.join(TEMP_DIR, f"thumb_{i}.jpg")
        # ã‚«ãƒƒãƒˆã®ä¸­é–“åœ°ç‚¹ã®æ™‚é–“ã‚’è¨ˆç®—
        mid_point = start_t + (duration / 2)
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜
        full_clip.save_frame(thumb_path, t=mid_point)

        # --- 2. éŸ³å£°ã®åˆ‡ã‚Šå‡ºã—ã¨æ–‡å­—èµ·ã“ã— ---
        audio_path = os.path.join(TEMP_DIR, f"audio_{i}.mp3")
        sub_clip = full_clip.subclip(start_t, end_t)
        
        transcript_text = "ï¼ˆãªã—ï¼‰"
        
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
        if sub_clip.audio is not None:
            try:
                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãå‡ºã—
                sub_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
                
                # Whisper APIã§æ–‡å­—èµ·ã“ã—
                with open(audio_path, "rb") as audio_file:
                    transcription = client.audio.transcriptions.create(
                        model="whisper-1", 
                        file=audio_file,
                        language="ja"
                    )
                transcript_text = transcription.text if transcription.text else "ï¼ˆãªã—ï¼‰"
            except Exception as e:
                # éŸ³å£°ãŒãªã„ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã‚¹ãƒ«ãƒ¼
                transcript_text = "ï¼ˆéŸ³å£°ãªã—/ã‚¨ãƒ©ãƒ¼ï¼‰"

        # --- 3. GPT-4o (Vision) ã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æ ---
        base64_image = encode_image(thumb_path)
        
        prompt = f"""
        ã“ã‚Œã¯å‹•ç”»ã®1ã‚«ãƒƒãƒˆã§ã™ã€‚
        éŸ³å£°ã®æ–‡å­—èµ·ã“ã—: ã€Œ{transcript_text}ã€
        
        ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ï¼š
        ã€çŠ¶æ³ã€‘: ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹è¦–è¦šçš„ãªçŠ¶æ³ï¼ˆèª°ãŒã€ã©ã“ã§ã€ä½•ã‚’ã—ã¦ã„ã‚‹ã‹ï¼‰
        ã€æ„å›³ã€‘: ã‚»ãƒªãƒ•ã¨ç”»åƒã‚’åˆã‚ã›ã¦ã€ã“ã®ã‚·ãƒ¼ãƒ³ãŒä½•ã‚’ä¼ãˆã¦ã„ã‚‹ã‹
        """

        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=300
            )
            analysis = response.choices[0].message.content
        except Exception as e:
            analysis = f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}"

        # çµæœã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        results.append({
            "ã‚«ãƒƒãƒˆNo": i+1,
            "é–‹å§‹": scene[0].get_timecode(),
            "çµ‚äº†": scene[1].get_timecode(),
            "ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹": thumb_path,
            "ã‚»ãƒªãƒ•": transcript_text,
            "AIåˆ†æ": analysis
        })
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
        progress_bar.progress((i + 1) / len(scenes))

    full_clip.close()
    status_text.text("åˆ†æå®Œäº†ï¼")
    return results

# --- Streamlit UI ---
st.set_page_config(page_title="AIã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼", layout="wide")
st.title("ğŸ¬ AIæ˜ åƒã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼")
st.markdown("æ˜ åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€**ã‚«ãƒƒãƒˆå‰²ã‚Šãƒ»æ–‡å­—èµ·ã“ã—ãƒ»å†…å®¹åˆ†æ**ã‚’å…¨è‡ªå‹•ã§è¡Œã„ã¾ã™ã€‚")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("è¨­å®š")
    api_key = st.text_input("OpenAI API Key", type="password")
    st.caption("â€»GPT-4oã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚")
    
    threshold = st.slider("ã‚«ãƒƒãƒˆæ¤œå‡ºæ„Ÿåº¦", 10.0, 60.0, 27.0)
    st.caption("å€¤ãŒå°ã•ã„ã»ã©æ•æ„Ÿã«ã‚«ãƒƒãƒˆã‚’æ¤œå‡ºã—ã¾ã™ã€‚")
    
    max_scenes_limit = st.number_input("æœ€å¤§åˆ†æã‚«ãƒƒãƒˆæ•°", value=5, min_value=1, max_value=50)

uploaded_file = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ« (MP4, MOV)", type=['mp4', 'mov'])

if uploaded_file and api_key:
    if st.button("ğŸš€ åˆ†æã‚¹ã‚¿ãƒ¼ãƒˆ"):
        try:
            data = process_video_and_analyze(api_key, uploaded_file, max_scenes=max_scenes_limit)
            
            # --- çµæœè¡¨ç¤º ---
            st.divider()
            st.subheader("ğŸ“‹ åˆ†æçµæœ")

            if data:
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆç”»åƒãƒ‘ã‚¹ã¯é™¤å¤–ï¼‰
                df_export = pd.DataFrame(data)
                csv = df_export.drop(columns=["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]).to_csv(index=False).encode('utf-8')
                
                st.download_button(
                    label="ğŸ’¾ CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name='cut_list.csv',
                    mime='text/csv',
                )

                # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è¡¨ç¤º
                for row in data:
                    with st.container():
                        col1, col2, col3 = st.columns([2, 2, 4])
                        
                        with col1:
                            # ç”»åƒã‚’è¡¨ç¤º
                            if os.path.exists(row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]):
                                st.image(row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"], use_column_width=True)
                            st.caption(f"{row['é–‹å§‹']} ã€œ {row['çµ‚äº†']}")
                        
                        with col2:
                            st.markdown("**ğŸ—£ï¸ ã‚»ãƒªãƒ• / éŸ³å£°**")
                            st.info(row["ã‚»ãƒªãƒ•"])
                        
                        with col3:
                            st.markdown("**ğŸ¤– AIåˆ†æ (è¦–è¦š+è´è¦š)**")
                            st.write(row["AIåˆ†æ"])
                        
                        st.divider()
            else:
                st.warning("ã‚·ãƒ¼ãƒ³ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚æ„Ÿåº¦ï¼ˆthresholdï¼‰ã‚’èª¿æ•´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

        except Exception as e:
            st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.warning("ãƒ’ãƒ³ãƒˆ: å¤§ãã™ãã‚‹å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚çŸ­ã„å‹•ç”»ã§è©¦ã—ã¦ãã ã•ã„ã€‚")

elif uploaded_file and not api_key:
    st.warning("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
