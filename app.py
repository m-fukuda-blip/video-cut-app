import streamlit as st
import os
import base64
import pandas as pd
import shutil
import zipfile
import io
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector, AdaptiveDetector
from moviepy.editor import VideoFileClip
from openai import OpenAI, RateLimitError

# --- è¨­å®š ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(BASE_DIR, "temp_data")

def init_temp_dir():
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
        except:
            pass
    os.makedirs(TEMP_DIR)

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def detect_scenes(video_path, threshold=27.0, min_scene_len=15, use_adaptive=False):
    """
    é«˜ç²¾åº¦ãªã‚·ãƒ¼ãƒ³æ¤œå‡º
    downscale_factor=1 : ç”»åƒã‚’ç¸®å°ã›ãšã€å…ƒã®ç”»è³ªã®ã¾ã¾å…¨ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¤å®šã™ã‚‹ï¼ˆé…ã„ãŒæ­£ç¢ºï¼‰
    """
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    
    # æ¤œå‡ºå™¨ã®é¸æŠ
    if use_adaptive:
        # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–: å‹•ãã®æ¿€ã—ã„æ˜ åƒã‚„ã€ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆã«å¼·ã„
        detector = AdaptiveDetector(adaptive_threshold=threshold, min_scene_len=min_scene_len, downscale_factor=1)
    else:
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: å˜ç´”ãªã‚«ãƒƒãƒˆå¤‰ã‚ã‚Šã«å¼·ã„ï¼ˆdownscale_factor=1ã§å…¨ç”»ç´ ãƒã‚§ãƒƒã‚¯ï¼‰
        detector = ContentDetector(threshold=threshold, min_scene_len=min_scene_len, downscale_factor=1)

    scene_manager.add_detector(detector)
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list(video_manager)
    return scene_list

def create_zip_file(data_list):
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        df = pd.DataFrame(data_list)
        df["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å"] = df["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"].apply(lambda x: os.path.basename(x))
        csv_data = df.drop(columns=["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]).to_csv(index=False).encode('utf-8')
        zf.writestr("cut_list.csv", csv_data)
        for row in data_list:
            img_path = row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]
            if os.path.exists(img_path):
                zf.write(img_path, arcname=f"images/{os.path.basename(img_path)}")
    return zip_buffer.getvalue()

def process_video_and_analyze(api_key, video_file, max_scenes, threshold, min_scene_len, use_adaptive):
    client = OpenAI(api_key=api_key)
    init_temp_dir()

    video_path = os.path.join(TEMP_DIR, "input_video.mp4")
    try:
        with open(video_path, "wb") as f:
            f.write(video_file.read())
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    st.info("âœ‚ï¸ ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­... (é«˜ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)")
    
    try:
        # ã“ã“ã§é«˜ç²¾åº¦ã®è¨­å®šã‚’æ¸¡ã™
        scenes = detect_scenes(video_path, threshold, min_scene_len, use_adaptive)
    except Exception as e:
        st.error(f"ã‚·ãƒ¼ãƒ³æ¤œå‡ºå¤±æ•—: {e}")
        return []

    st.write(f"åˆè¨ˆ **{len(scenes)}** ã‚«ãƒƒãƒˆæ¤œå‡ºã—ã¾ã—ãŸã€‚")
    
    if len(scenes) > max_scenes:
        st.warning(f"âš ï¸ ãƒ‡ãƒ¢åˆ¶é™: æœ€åˆã® {max_scenes} ã‚«ãƒƒãƒˆã®ã¿å‡¦ç†ã—ã¾ã™ã€‚")
        scenes = scenes[:max_scenes]

    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    full_clip = VideoFileClip(video_path)

    for i, scene in enumerate(scenes):
        start_t = scene[0].get_seconds()
        end_t = scene[1].get_seconds()
        duration = end_t - start_t
        
        status_text.text(f"AIåˆ†æä¸­: ã‚«ãƒƒãƒˆ {i+1}/{len(scenes)}")
        
        thumb_filename = f"cut_{i+1:03}.jpg"
        thumb_path = os.path.join(TEMP_DIR, thumb_filename)
        
        # ã€å·¥å¤«ã€‘çœŸã‚“ä¸­ã ã‘ã§ãªãã€å°‘ã—å‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚‚ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒ–ãƒ¬ã¦ãªã„ã‹è¦‹ã‚‹å®Ÿè£…ã‚‚å¯èƒ½ã ãŒ
        # ä»Šå›ã¯è¨ˆç®—é‡ç¯€ç´„ã®ãŸã‚ã€Œé–‹å§‹ã‹ã‚‰20%ã€ã¨ã€Œ50%ã€ã®åœ°ç‚¹ã§å®‰å…¨ãªæ–¹ã‚’ã¨ã‚‹ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯
        capture_point = start_t + (duration * 0.5) # çœŸã‚“ä¸­
        
        try:
            full_clip.save_frame(thumb_path, t=capture_point)
        except:
            continue

        # éŸ³å£°å‡¦ç†
        audio_path = os.path.join(TEMP_DIR, f"audio_{i}.mp3")
        sub_clip = full_clip.subclip(start_t, end_t)
        transcript_text = "ï¼ˆãªã—ï¼‰"

        if sub_clip.audio is not None:
            try:
                sub_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
                with open(audio_path, "rb") as audio_file:
                    transcription = client.audio.transcriptions.create(
                        model="whisper-1", file=audio_file, language="ja"
                    )
                transcript_text = transcription.text if transcription.text else "ï¼ˆãªã—ï¼‰"
            except RateLimitError:
                transcript_text = "âŒ Error: æ®‹é«˜ä¸è¶³"
            except Exception:
                transcript_text = "ï¼ˆéŸ³å£°ã‚¨ãƒ©ãƒ¼ï¼‰"

        # GPT-4o
        base64_image = encode_image(thumb_path)
        prompt = f"æ–‡å­—èµ·ã“ã—:ã€Œ{transcript_text}ã€ã€‚ã“ã®ã‚«ãƒƒãƒˆã®çŠ¶æ³ã¨æ„å›³ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ã€‚"
        
        analysis = ""
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                    ]}
                ],
                max_tokens=200
            )
            analysis = response.choices[0].message.content
        except RateLimitError:
            analysis = "âŒ Error: ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆæ®‹é«˜ä¸è¶³"
            st.error("âš ï¸ OpenAIã®åˆ©ç”¨æ è¶…é (429) ã§ã™ã€‚Billingè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            analysis = f"ã‚¨ãƒ©ãƒ¼: {e}"

        results.append({
            "ã‚«ãƒƒãƒˆNo": i+1,
            "é–‹å§‹": scene[0].get_timecode(),
            "çµ‚äº†": scene[1].get_timecode(),
            "ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹": thumb_path,
            "ã‚»ãƒªãƒ•": transcript_text,
            "AIåˆ†æ": analysis
        })
        progress_bar.progress((i + 1) / len(scenes))

    full_clip.close()
    status_text.text("å®Œäº†ï¼")
    return results

# --- UI ---
st.set_page_config(page_title="AIã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼ Pro", layout="wide")
st.title("ğŸ¬ AIæ˜ åƒã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼ (é«˜ç²¾åº¦ç‰ˆ)")

with st.sidebar:
    api_key = st.text_input("OpenAI API Key", type="password")
    
    st.divider()
    st.header("æ¤œå‡ºè¨­å®š")
    
    # æ–°æ©Ÿèƒ½: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ
    use_adaptive = st.checkbox("Adaptiveãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ã†", value=False, help="ãƒ•ã‚§ãƒ¼ãƒ‰ã‚„å‹•ãã®æ¿€ã—ã„æ˜ åƒã«å¼·ã„ã§ã™ãŒã€å‡¦ç†ãŒé…ããªã‚Šã¾ã™ã€‚")
    
    threshold = st.slider("æ„Ÿåº¦ (Threshold)", 10.0, 60.0, 27.0, help="å€¤ã‚’ä¸‹ã’ã‚‹ã¨ç´°ã‹ã„å¤‰åŒ–ã‚‚æ¤œå‡ºã—ã¾ã™ï¼ˆéå‰°æ¤œå‡ºã«æ³¨æ„ï¼‰ã€‚")
    
    # æ–°æ©Ÿèƒ½: æœ€å°ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
    min_scene_len = st.number_input("æœ€å°ã‚«ãƒƒãƒˆé•· (ãƒ•ãƒ¬ãƒ¼ãƒ )", value=15, min_value=1, help="ã“ã‚Œã‚ˆã‚ŠçŸ­ã„ã‚«ãƒƒãƒˆã¯ãƒã‚¤ã‚ºã¨ã—ã¦ç„¡è¦–ã—ã¾ã™ï¼ˆ15ãƒ•ãƒ¬ãƒ¼ãƒ â‰’0.5ç§’ï¼‰ã€‚")
    
    max_scenes_limit = st.number_input("æœ€å¤§åˆ†ææ•°", 5, 100, 10)

uploaded_file = st.file_uploader("å‹•ç”» (MP4)", type=['mp4', 'mov'])

if uploaded_file and api_key:
    if st.button("ğŸš€ åˆ†æã‚¹ã‚¿ãƒ¼ãƒˆ"):
        data = process_video_and_analyze(
            api_key, 
            uploaded_file, 
            max_scenes_limit,
            threshold,
            min_scene_len,
            use_adaptive
        )
        
        if data:
            st.success("åˆ†æå®Œäº†ï¼")
            
            zip_bytes = create_zip_file(data)
            st.download_button(
                label="ğŸ“¦ çµæœã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV+ç”»åƒ)",
                data=zip_bytes,
                file_name="cut_analysis_pro.zip",
                mime="application/zip"
            )

            for row in data:
                col1, col2, col3 = st.columns([2, 2, 4])
                with col1:
                    st.image(row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"])
                    st.caption(f"{row['é–‹å§‹']} - {row['çµ‚äº†']}")
                with col2:
                    st.write(f"ğŸ—£ {row['ã‚»ãƒªãƒ•']}")
                with col3:
                    st.write(f"ğŸ¤– {row['AIåˆ†æ']}")
                st.divider()
