import streamlit as st
import os
import base64
import pandas as pd
import shutil
import zipfile
import io
import cv2
import numpy as np
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector, AdaptiveDetector
from moviepy.editor import VideoFileClip
from openai import OpenAI, RateLimitError

# --- è¨­å®š ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(BASE_DIR, "temp_data")

# é¡”èªè­˜ç”¨ã®åˆ†é¡å™¨ï¼ˆOpenCVæ¨™æº–ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰
face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
face_cascade = cv2.CascadeClassifier(face_cascade_path)

def init_temp_dir():
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
        except:
            pass
    os.makedirs(TEMP_DIR)

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def evaluate_frame(frame_img):
    """
    ãƒ•ãƒ¬ãƒ¼ãƒ ã®å“è³ªã‚’ã‚¹ã‚³ã‚¢åŒ–ã™ã‚‹é–¢æ•°
    1. ãƒ–ãƒ¬ã¦ã„ãªã„ã‹ï¼ˆé®®æ˜åº¦ï¼‰
    2. äººã®é¡”ãŒæ˜ ã£ã¦ã„ã‚‹ã‹
    """
    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
    gray = cv2.cvtColor(frame_img, cv2.COLOR_RGB2GRAY)
    
    # 1. é®®æ˜åº¦ã‚¹ã‚³ã‚¢ï¼ˆãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³åˆ†æ•£ï¼‰
    # æ•°å€¤ãŒå¤§ãã„ã»ã©ã‚¨ãƒƒã‚¸ãŒåŠ¹ã„ã¦ã„ã‚‹ï¼ˆãƒ”ãƒ³ãƒˆãŒåˆã£ã¦ã„ã‚‹ï¼‰
    sharpness_score = cv2.Laplacian(gray, cv2.CV_64F).var()
    
    # 2. é¡”æ¤œå‡ºãƒœãƒ¼ãƒŠã‚¹
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    face_bonus = 0
    if len(faces) > 0:
        # é¡”ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ã€é®®æ˜åº¦ã«é–¢ã‚ã‚‰ãšå¤§ããåŠ ç‚¹ï¼ˆ+300ç‚¹ï¼‰
        # ã“ã‚Œã«ã‚ˆã‚Šã€Œãƒ–ãƒ¬ã¦ãªã„é¢¨æ™¯ã€ã‚ˆã‚Šã€Œå¤šå°‘ãƒ–ãƒ¬ã¦ã¦ã‚‚äººãŒã„ã‚‹ã€ã‚’å„ªå…ˆã™ã‚‹å‚¾å‘ã«ã™ã‚‹
        face_bonus = 300
    
    total_score = sharpness_score + face_bonus
    return total_score

def save_best_frame(clip, start, end, output_path):
    """
    æŒ‡å®šåŒºé–“ã‹ã‚‰ãƒ™ã‚¹ãƒˆãªï¼ˆé®®æ˜ã‹ã¤é¡”ãŒã‚ã‚‹ï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¢ã—ã¦ä¿å­˜
    """
    duration = end - start
    
    # ãƒã‚§ãƒƒã‚¯ã™ã‚‹å€™è£œã®æ•°ï¼ˆå¤šã„ã»ã©æ­£ç¢ºã ãŒé…ããªã‚‹ï¼‰
    # 0.5ç§’ä»¥ä¸‹ã®çŸ­ã„ã‚«ãƒƒãƒˆã¯çœŸã‚“ä¸­1ç™ºå‹è² 
    if duration < 0.5:
        t_candidates = [start + duration/2]
    else:
        # å§‹ç‚¹ã¨çµ‚ç‚¹ã®ã‚®ãƒªã‚®ãƒªã¯é¿ã‘ã¦ã€å‡ç­‰ã«5ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        t_candidates = np.linspace(start + 0.1, end - 0.1, num=5)

    best_score = -1
    best_t = t_candidates[0]

    for t in t_candidates:
        try:
            # moviepyã§ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾— (numpy array)
            frame = clip.get_frame(t)
            score = evaluate_frame(frame)
            
            if score > best_score:
                best_score = score
                best_t = t
        except:
            continue
    
    # ãƒ™ã‚¹ãƒˆãªæ™‚é–“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜
    clip.save_frame(output_path, t=best_t)


def detect_scenes(video_path, threshold=27.0, min_scene_len=15, use_adaptive=False):
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    
    if use_adaptive:
        detector = AdaptiveDetector(adaptive_threshold=threshold, min_scene_len=min_scene_len)
    else:
        detector = ContentDetector(threshold=threshold, min_scene_len=min_scene_len)

    scene_manager.add_detector(detector)
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list(video_manager)
    return scene_list

def create_zip_file(data_list):
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        df = pd.DataFrame(data_list)
        df["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å"] = df["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"].apply(lambda x: os.path.basename(x))
        csv_data = df.drop(columns=["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]).to_csv(index=False).encode('utf-8')
        zf.writestr("cut_list.csv", csv_data)
        for row in data_list:
            img_path = row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"]
            if os.path.exists(img_path):
                zf.write(img_path, arcname=f"images/{os.path.basename(img_path)}")
    return zip_buffer.getvalue()

def process_video_and_analyze(api_key, video_file, max_scenes, threshold, min_scene_len, use_adaptive):
    client = OpenAI(api_key=api_key)
    init_temp_dir()

    video_path = os.path.join(TEMP_DIR, "input_video.mp4")
    try:
        with open(video_path, "wb") as f:
            f.write(video_file.read())
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    st.info("âœ‚ï¸ ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­...")
    
    try:
        scenes = detect_scenes(video_path, threshold, min_scene_len, use_adaptive)
    except Exception as e:
        st.error(f"ã‚·ãƒ¼ãƒ³æ¤œå‡ºå¤±æ•—: {e}")
        return []

    st.write(f"åˆè¨ˆ **{len(scenes)}** ã‚«ãƒƒãƒˆæ¤œå‡ºã€‚ãƒ™ã‚¹ãƒˆã‚·ãƒ§ãƒƒãƒˆé¸æŠœã‚’é–‹å§‹ã—ã¾ã™...")
    
    if len(scenes) > max_scenes:
        st.warning(f"âš ï¸ ãƒ‡ãƒ¢åˆ¶é™: æœ€åˆã® {max_scenes} ã‚«ãƒƒãƒˆã®ã¿å‡¦ç†ã—ã¾ã™ã€‚")
        scenes = scenes[:max_scenes]

    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    full_clip = VideoFileClip(video_path)

    for i, scene in enumerate(scenes):
        start_t = scene[0].get_seconds()
        end_t = scene[1].get_seconds()
        duration = end_t - start_t
        
        status_text.text(f"åˆ†æä¸­: ã‚«ãƒƒãƒˆ {i+1}/{len(scenes)} (ãƒ™ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ æ¢ç´¢ä¸­...)")
        
        thumb_filename = f"cut_{i+1:03}.jpg"
        thumb_path = os.path.join(TEMP_DIR, thumb_filename)
        
        # --- æ”¹è‰¯ç‚¹: ãƒ™ã‚¹ãƒˆã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½ ---
        try:
            save_best_frame(full_clip, start_t, end_t, thumb_path)
        except Exception as e:
            st.warning(f"ãƒ•ãƒ¬ãƒ¼ãƒ ä¿å­˜ã‚¨ãƒ©ãƒ¼(skip): {e}")
            continue
        # -----------------------------

        # éŸ³å£°å‡¦ç†
        audio_path = os.path.join(TEMP_DIR, f"audio_{i}.mp3")
        sub_clip = full_clip.subclip(start_t, end_t)
        transcript_text = "ï¼ˆãªã—ï¼‰"

        if sub_clip.audio is not None:
            try:
                sub_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
                with open(audio_path, "rb") as audio_file:
                    transcription = client.audio.transcriptions.create(
                        model="whisper-1", file=audio_file, language="ja"
                    )
                transcript_text = transcription.text if transcription.text else "ï¼ˆãªã—ï¼‰"
            except RateLimitError:
                transcript_text = "âŒ Error: æ®‹é«˜ä¸è¶³"
            except Exception:
                transcript_text = "ï¼ˆéŸ³å£°ã‚¨ãƒ©ãƒ¼ï¼‰"

        # GPT-4o
        base64_image = encode_image(thumb_path)
        prompt = f"æ–‡å­—èµ·ã“ã—:ã€Œ{transcript_text}ã€ã€‚ã“ã®ã‚«ãƒƒãƒˆã®çŠ¶æ³ã¨æ„å›³ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ã€‚"
        
        analysis = ""
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                    ]}
                ],
                max_tokens=200
            )
            analysis = response.choices[0].message.content
        except RateLimitError:
            analysis = "âŒ Error: ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆæ®‹é«˜ä¸è¶³"
            st.error("âš ï¸ OpenAIã®åˆ©ç”¨æ è¶…é (429) ã§ã™ã€‚")
        except Exception as e:
            analysis = f"ã‚¨ãƒ©ãƒ¼: {e}"

        results.append({
            "ã‚«ãƒƒãƒˆNo": i+1,
            "é–‹å§‹": scene[0].get_timecode(),
            "çµ‚äº†": scene[1].get_timecode(),
            "ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹": thumb_path,
            "ã‚»ãƒªãƒ•": transcript_text,
            "AIåˆ†æ": analysis
        })
        progress_bar.progress((i + 1) / len(scenes))

    full_clip.close()
    status_text.text("å®Œäº†ï¼")
    return results

# --- UI ---
st.set_page_config(page_title="AIã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼ BestShot", layout="wide")
st.title("ğŸ¬ AIæ˜ åƒã‚«ãƒƒãƒˆè¡¨ãƒ¡ãƒ¼ã‚«ãƒ¼ (ãƒ™ã‚¹ãƒˆã‚·ãƒ§ãƒƒãƒˆç‰ˆ)")

with st.sidebar:
    api_key = st.text_input("OpenAI API Key", type="password")
    
    st.divider()
    st.header("æ¤œå‡ºè¨­å®š")
    use_adaptive = st.checkbox("Adaptiveãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ã†", value=False)
    threshold = st.slider("æ„Ÿåº¦ (Threshold)", 10.0, 60.0, 27.0)
    min_scene_len = st.number_input("æœ€å°ã‚«ãƒƒãƒˆé•· (ãƒ•ãƒ¬ãƒ¼ãƒ )", value=15, min_value=1)
    max_scenes_limit = st.number_input("æœ€å¤§åˆ†ææ•°", 5, 100, 10)

uploaded_file = st.file_uploader("å‹•ç”» (MP4)", type=['mp4', 'mov'])

if uploaded_file and api_key:
    if st.button("ğŸš€ åˆ†æã‚¹ã‚¿ãƒ¼ãƒˆ"):
        data = process_video_and_analyze(
            api_key, 
            uploaded_file, 
            max_scenes_limit,
            threshold,
            min_scene_len,
            use_adaptive
        )
        
        if data:
            st.success("åˆ†æå®Œäº†ï¼")
            
            zip_bytes = create_zip_file(data)
            st.download_button(
                label="ğŸ“¦ çµæœã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV+ç”»åƒ)",
                data=zip_bytes,
                file_name="cut_analysis_best.zip",
                mime="application/zip"
            )

            for row in data:
                col1, col2, col3 = st.columns([2, 2, 4])
                with col1:
                    st.image(row["ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹"])
                    st.caption(f"{row['é–‹å§‹']} - {row['çµ‚äº†']}")
                with col2:
                    st.write(f"ğŸ—£ {row['ã‚»ãƒªãƒ•']}")
                with col3:
                    st.write(f"ğŸ¤– {row['AIåˆ†æ']}")
                st.divider()
